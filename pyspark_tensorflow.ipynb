{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadmarajBhat/Real-Time-Analytics-on-Hadoop-Notes/blob/master/pyspark_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz4CQmBXBHml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4ce16829-f76f-4d75-c9c6-8cbe6f8cecce"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 131kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 37.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=833304e0360467d406a8228d299f1507184cda6bfb04bc1ec4e5ba5c52aba7eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1Z2TmZFBc3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7a1lSl7BrQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x,train_y), (test_x, test_y) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ile5SMUSCA-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5dd8c50-fdda-42eb-822e-2d36cfa00dba"
      },
      "source": [
        "train_x.shape,train_y.shape, test_x.shape, test_y.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olGC6S-bCQl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "429cba70-d2c5-4045-c97a-5e4c65181049"
      },
      "source": [
        "pd_train= pd.DataFrame(train_x.reshape(60000,784))\n",
        "pd_test = pd.DataFrame(test_x.reshape(10000,784))\n",
        "pd_train.shape,pd_test.shape\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784), (10000, 784))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn-m1DHPHNAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "3d1e2dab-bb95-4928-c32a-1537ed50f6ad"
      },
      "source": [
        "pd_train.values *.1"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ5eyAGLKyPM",
        "colab_type": "text"
      },
      "source": [
        "Note that we had to multiply .1 to convert the data to float else not accepted in the tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb08ox-CCI1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "5c77bfe8-8e2a-4e06-be52-918114489a8a"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    #tf.keras.layers.Flatten(input_shape=(28,28))\n",
        "    tf.keras.layers.Flatten()\n",
        "    ,tf.keras.layers.Dense(256,activation=\"relu\")\n",
        "    ,tf.keras.layers.Dense(128,activation=\"relu\")\n",
        "    ,tf.keras.layers.Dense(10,activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"Adam\"\n",
        "              , loss=\"sparse_categorical_crossentropy\"\n",
        "              , metrics=['accuracy']\n",
        "             )\n",
        "\n",
        "#model.fit(train,train_y, epochs=6)\n",
        "model.fit(pd_train.values *.1,train_y,epochs=6)\n",
        "model.evaluate(test_x,test_y)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "60000/60000 [==============================] - 6s 99us/sample - loss: 0.2867 - acc: 0.9256\n",
            "Epoch 2/6\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.1264 - acc: 0.9624\n",
            "Epoch 3/6\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0967 - acc: 0.9718\n",
            "Epoch 4/6\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0790 - acc: 0.9758\n",
            "Epoch 5/6\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0692 - acc: 0.9791\n",
            "Epoch 6/6\n",
            "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0592 - acc: 0.9823\n",
            "10000/10000 [==============================] - 0s 44us/sample - loss: 0.8385 - acc: 0.9752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8385225829925507, 0.9752]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPuyi6f0LKnO",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow is being fed with panda which indicates that entire data is loaded. This is not a good fit for Big Data case. We need the tensorflow to work on the RDD or Df rather than panda df. Hence spark read --> convert to panda --> tensorflow pipeline fails at the second stage. For this we have https://github.com/maxpumperla/elephas\n",
        "\n",
        "\n",
        "Elephas: Take spark dataframe and trains it for the keras model. But the example does not talk about tensorflow keras but keras independent version. Not sure if tensorflow keras supports it.\n",
        "\n",
        "Why we have to worry about tf.keras or keras as independent one ? \n",
        "  - tensorflow official supports keras and hence product would have enhancements\n",
        "  - tensorflow has distributed stratergy to support distributed training. elephas has it too but may be tensorflow is better tested one becuase of larger community of dev and tester.\n",
        "  - tensorflow has official distribution on hadoop and hence better than smaller community like elephas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AbTbc-XEflr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}