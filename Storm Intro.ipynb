{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Analytics with Apache Storm\n",
    "\n",
    "\n",
    "\n",
    "### Chapter 1: Basics of Storm\n",
    "* **Real Time Analytics** : Broadly it can be of 2 type\n",
    "    * Streaming : Data is being analysed as and when it is produced\n",
    "    * Interactive : Store the data and provid the result of the analysis when queried.\n",
    "\n",
    "* **Storm**:\n",
    "    * real time analytics platform for streams of data.\n",
    "    * Hadoop does the batch processing and storm does the real time processing.\n",
    "        * My study on the differences between them indicates that the choice of the platform depends on the processing rate\n",
    "            * i.e. if you need second wise \"real time processing\" then you can go with spark streaming and less than that you would need storm.\n",
    "            \n",
    "            \n",
    "    * SPOUTS: Data source (db, kafka, kestrel)\n",
    "    * BOLT: operations on data ( filtering/aggregaion/join/transformations )\n",
    "    * Tuple: immutable ordered list\n",
    "    * Topology: DAG, (no loop), vertices are computation and edges are streams.\n",
    "    \n",
    "    * Grouping:\n",
    "        * Since we can have multiple layers of bolts, there needs a mapping of source to bolts and bolts themselves. \n",
    "        \n",
    "        * Shuffle Grouping: randomly sends tuples to next stage bolts.\n",
    "        * Field Grouping: Groups tuples by single or multiple fields and target the next stage bolts on the basis of it\n",
    "        * All Grouping: Sends all tuples to all next stage bolts.\n",
    "        * Global Grouping: Sends all tuples to one single next stage bolt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storm is outdated and I also see that its last release was June 2018 , an year back.\n",
    "\n",
    "I went through this post; https://medium.com/@chandanbaranwal/spark-streaming-vs-flink-vs-storm-vs-kafka-streams-vs-samza-choose-your-stream-processing-91ea3f04675b\n",
    "\n",
    "I guess, Spark Stream is what I have learn because \n",
    "    * though kafka stream and flink are emergenging and gaining popularity, it is yet prove in the market and \n",
    "    * spark stream does not require any addition set up.\n",
    "    * spark can be installed locally ubuntu or windows.\n",
    "        * First Download winutils.exe: https://github.com/steveloughran/winutils/blob/master/hadoop-3.0.0/bin/winutils.exe\n",
    "        * Keep in a location; ...\\bin\\winutils.exe\n",
    "        * Mention the location has HADOOP_HOME in environment variable\n",
    "            * HADOOP_HOME = C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\winutils\n",
    "            \n",
    "        * from conda prompt (environment) enter; conda install pyspark\n",
    "        * you are ready use spark on windows system.\n",
    "        \n",
    "        * Use local files prefixing \"file:////\"\n",
    "        sdf = spark.read.csv(\"file:///C:/Users/Dell/Desktop/Hadoop/Spark/temp.csv\")\n",
    "        sdf.write.parquet(\"C:/Users/Dell/Desktop/Hadoop/Spark/temp_parquet\")\n",
    "\n",
    "\n",
    "        sdf = spark.read.parquet(\"file:///C:/Users/Dell/Desktop/Hadoop/Spark/temp_parquet_6\")\n",
    "        sdf.show()\n",
    "            \n",
    "        * if you dont set the environment variable and winutils , pyspark launch issues below warning.\n",
    "            \n",
    "```\n",
    "19/05/05 15:23:02 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path\n",
    "java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n",
    "        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)\n",
    "        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)\n",
    "        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:387)\n",
    "        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\n",
    "        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)\n",
    "        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273)\n",
    "        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261)\n",
    "        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:791)\n",
    "        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:761)\n",
    "        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:634)\n",
    "        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)\n",
    "        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)\n",
    "        at scala.Option.getOrElse(Option.scala:121)\n",
    "        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2422)\n",
    "        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)\n",
    "        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:359)\n",
    "        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1(SparkSubmit.scala:359)\n",
    "        at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:367)\n",
    "        at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:367)\n",
    "        at scala.Option.map(Option.scala:146)\n",
    "        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:366)\n",
    "        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:143)\n",
    "        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
    "        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)\n",
    "        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)\n",
    "        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
    "19/05/05 15:23:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
