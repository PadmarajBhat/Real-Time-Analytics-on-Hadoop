{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MLlib\").getOrCreate()\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "print(df.show())\n",
    "\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))\n",
    "```\n",
    "\n",
    "* Basic correlation display\n",
    "* I guess we have to focus on vectors here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+--------------------+\n",
    "|            features|\n",
    "+--------------------+\n",
    "|(4,[0,3],[1.0,-2.0])|\n",
    "|   [4.0,5.0,0.0,3.0]|\n",
    "|   [6.0,7.0,0.0,8.0]|\n",
    "| (4,[0,3],[9.0,1.0])|\n",
    "+--------------------+\n",
    "\n",
    "None\n",
    "[Stage 4:>                                                          (0 + 4) / 4]19/05/14 22:37:50 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
    "19/05/14 22:37:50 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
    "19/05/14 22:37:51 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
    "Pearson correlation matrix:\n",
    "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
    "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
    "             [       nan,        nan, 1.        ,        nan],\n",
    "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
    "19/05/14 22:37:58 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
    "Spearman correlation matrix:\n",
    "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
    "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
    "             [       nan,        nan, 1.        ,        nan],\n",
    "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n",
    "SUCCESS: The process with PID 3768 (child process of PID 11404) has been terminated.\n",
    "SUCCESS: The process with PID 11404 (child process of PID 212) has been terminated.\n",
    "SUCCESS: The process with PID 212 (child process of PID 6828) has been terminated.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is vectors?\n",
    "* They are equivalent to numpy arrays or lists in python : array of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 0, 'abcd', list([0, 1, 2, 3])], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array(['a', 0, \"abcd\", [0,1,2,3]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['d', 0, 'abcd', list([0, 1, 2, 3])], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] = 'd'\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chi-squared Test\n",
    "\n",
    "* Chi-squared : pronounced as kai squared, represented as (X square): does a hypothesis test between observed and expected with a formula : SUM ( (observed - expected)^2 / (expected) )\n",
    "    * https://www.youtube.com/watch?v=1Ldl5Zfcm1Y\n",
    "    * scipy package return a p value and that value if less that the permitted tolerance (alpha) then we can reject null hypothesis.\n",
    "    * this is only for categorical data\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html\n",
    "        \n",
    "        \n",
    "    * code as in https://spark.apache.org/docs/latest/ml-statistics.html\n",
    "        * What is Null Hypothesis: The null hypothesis is that the occurrence of the outcomes is statistically independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output:\n",
    "pValues: [0.6872892787909721,0.6822703303362126]\n",
    " \n",
    "degreesOfFreedom: [2, 3]\n",
    "\n",
    "statistics: [0.75,1.5]\n",
    "```\n",
    ">>> r\n",
    "Row(pValues=DenseVector([0.6873, 0.6823]), degreesOfFreedom=[2, 3], statistics=DenseVector([0.75, 1.5]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> from pyspark.ml.stat import Summarizer\n",
    ">>> from pyspark.sql import Row\n",
    ">>> from pyspark.ml.linalg import Vectors\n",
    ">>>\n",
    ">>> df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "...                      Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    ">>>\n",
    ">>> # create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "... summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
    ">>>\n",
    ">>> # compute statistics for multiple metrics with weight\n",
    "... df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
    "+-----------------------------------+\n",
    "|aggregate_metrics(features, weight)|\n",
    "+-----------------------------------+\n",
    "|[[1.0,1.0,1.0], 1]                 |\n",
    "+-----------------------------------+\n",
    "\n",
    ">>>\n",
    ">>> # compute statistics for multiple metrics without weight\n",
    "... df.select(summarizer.summary(df.features)).show(truncate=False)\n",
    "+--------------------------------+\n",
    "|aggregate_metrics(features, 1.0)|\n",
    "+--------------------------------+\n",
    "|[[1.0,1.5,2.0], 2]              |\n",
    "+--------------------------------+\n",
    "\n",
    ">>>\n",
    ">>> # compute statistics for single metric \"mean\" with weight\n",
    "... df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "+--------------+\n",
    "|mean(features)|\n",
    "+--------------+\n",
    "|[1.0,1.0,1.0] |\n",
    "+--------------+\n",
    "\n",
    ">>>\n",
    ">>> # compute statistics for single metric \"mean\" without weight\n",
    "... df.select(Summarizer.mean(df.features)).show(truncate=False)\n",
    "+--------------+\n",
    "|mean(features)|\n",
    "+--------------+\n",
    "|[1.0,1.5,2.0] |\n",
    "+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"dummy\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,2,3),(4,5,6),(7,8,9)],[\"a\",\"b\",\"c\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+\n",
      "|summary|  a|  b|  c|\n",
      "+-------+---+---+---+\n",
      "|  count|  3|  3|  3|\n",
      "|   mean|4.0|5.0|6.0|\n",
      "| stddev|3.0|3.0|3.0|\n",
      "|    min|  1|  2|  3|\n",
      "|    max|  7|  8|  9|\n",
      "+-------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
