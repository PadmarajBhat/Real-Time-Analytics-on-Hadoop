{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sample.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadmarajBhat/Real-Time-Analytics-on-Hadoop/blob/master/Sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL-fjCGFb7p1",
        "colab_type": "code",
        "outputId": "97e9e64b-d07d-4b34-fc0d-e4eb701f3cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "!pip install -U pyspark\n",
        "!pip install -U pyarrow\n",
        "!sudo apt install openjdk-8-jdk\n",
        "!sudo update-alternatives --config java"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n",
            "Requirement already up-to-date: pyarrow in /usr/local/lib/python3.6/dist-packages (0.13.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.16.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jdk is already the newest version (8u212-b03-0ubuntu1.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "  0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "* 2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ50ia1Xc8Wc",
        "colab_type": "code",
        "outputId": "7a53dbdd-a04b-4262-e66f-5264705a6044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"MLlib\").getOrCreate()\n",
        "a = [(0,1),(0,2),(1,1),(1,2),(2,1),(2,2),(3,1),(3,2)]\n",
        "b = spark.createDataFrame(a,[\"class\",\"num\"])\n",
        "b.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+\n",
            "|class|num|\n",
            "+-----+---+\n",
            "|    0|  1|\n",
            "|    0|  2|\n",
            "|    1|  1|\n",
            "|    1|  2|\n",
            "|    2|  1|\n",
            "|    2|  2|\n",
            "|    3|  1|\n",
            "|    3|  2|\n",
            "+-----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU8735T2dl83",
        "colab_type": "code",
        "outputId": "95967586-dba0-438d-b66d-f0682259759d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b= b.withColumn(\"num\", b['num']/b['num'])\n",
        "b.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+\n",
            "|class|num|\n",
            "+-----+---+\n",
            "|    0|1.0|\n",
            "|    0|1.0|\n",
            "|    1|1.0|\n",
            "|    1|1.0|\n",
            "|    2|1.0|\n",
            "|    2|1.0|\n",
            "|    3|1.0|\n",
            "|    3|1.0|\n",
            "+-----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp__m1ond5Ws",
        "colab_type": "code",
        "outputId": "3e25cf6c-4eea-4eca-baad-e373208cb852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b=b.withColumn(\"new\", b['class']+b['num'])\n",
        "b.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+\n",
            "|class|num|new|\n",
            "+-----+---+---+\n",
            "|    0|1.0|1.0|\n",
            "|    0|1.0|1.0|\n",
            "|    1|1.0|2.0|\n",
            "|    1|1.0|2.0|\n",
            "|    2|1.0|3.0|\n",
            "|    2|1.0|3.0|\n",
            "|    3|1.0|4.0|\n",
            "|    3|1.0|4.0|\n",
            "+-----+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JBuVXEHeKUV",
        "colab_type": "code",
        "outputId": "66364ebe-c826-4075-fed6-c4f80387f1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "\n",
        "b.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+\n",
            "|class|num|new|\n",
            "+-----+---+---+\n",
            "|    0|1.0|1.0|\n",
            "|    0|1.0|1.0|\n",
            "|    1|1.0|2.0|\n",
            "|    1|1.0|2.0|\n",
            "|    2|1.0|3.0|\n",
            "|    2|1.0|3.0|\n",
            "|    3|1.0|4.0|\n",
            "|    3|1.0|4.0|\n",
            "+-----+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-kWrwvweTEt",
        "colab_type": "code",
        "outputId": "b2e42c9f-4b32-421b-fa9d-20d0b3b4b843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!echo $PATH"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_xW3TFjfuSp",
        "colab_type": "code",
        "outputId": "e31096d7-b72e-484c-aa38-0ed22e471851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               63\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2300.000\n",
            "BogoMIPS:            4600.00\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            46080K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat arch_capabilities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTd-AZIlf-ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!less /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-C-RrBF-tdD",
        "colab_type": "text"
      },
      "source": [
        "Nice Article on UDFs: https://www.slideshare.net/ueshin/apache-arrow-and-pandas-udf-on-apache-spark\n",
        "\n",
        "* When is SCALAR needed?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wKqXCSegRGr",
        "colab_type": "code",
        "outputId": "75ae18f4-2ab8-4d85-e191-214ef38709d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b.select('num').show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "|num|\n",
            "+---+\n",
            "|1.0|\n",
            "|1.0|\n",
            "|1.0|\n",
            "|1.0|\n",
            "|1.0|\n",
            "|1.0|\n",
            "|1.0|\n",
            "|1.0|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiJFzpA8HY_J",
        "colab_type": "code",
        "outputId": "d9a2b196-2008-4852-d681-96a6ff230700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b.withColumn(\"num_2\", b[\"num\"]* 5).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+-----+\n",
            "|class|num|new|num_2|\n",
            "+-----+---+---+-----+\n",
            "|    0|1.0|1.0|  5.0|\n",
            "|    0|1.0|1.0|  5.0|\n",
            "|    1|1.0|2.0|  5.0|\n",
            "|    1|1.0|2.0|  5.0|\n",
            "|    2|1.0|3.0|  5.0|\n",
            "|    2|1.0|3.0|  5.0|\n",
            "|    3|1.0|4.0|  5.0|\n",
            "|    3|1.0|4.0|  5.0|\n",
            "+-----+---+---+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9IqjEqwIWJU",
        "colab_type": "code",
        "outputId": "5ec28775-b39e-493a-c866-956828aed1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "b.withColumn(\"str(num)\", str(b['num']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5ecaf077806f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"str(num)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \"\"\"\n\u001b[0;32m-> 1988\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlVlmRcSJJvH",
        "colab_type": "text"
      },
      "source": [
        "Create UDF to convert number to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbfq63O_I5iN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import col, pandas_udf, concat, lit\n",
        "from pyspark.sql.types import LongType, StringType\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "def num_to_string(a):\n",
        "    b= a.apply(lambda x: str(x)+\"_1\")\n",
        "    #print(b)\n",
        "    return b\n",
        "\n",
        "numToString = pandas_udf(num_to_string, returnType=StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY59cMc_J0GU",
        "colab_type": "code",
        "outputId": "3541f87d-e549-4a28-b2ad-e907a8f71334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b.withColumn(\"new_num\", numToString(col('num'))).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+-------+\n",
            "|class|num|new|new_num|\n",
            "+-----+---+---+-------+\n",
            "|    0|1.0|1.0|  1.0_1|\n",
            "|    0|1.0|1.0|  1.0_1|\n",
            "|    1|1.0|2.0|  1.0_1|\n",
            "|    1|1.0|2.0|  1.0_1|\n",
            "|    2|1.0|3.0|  1.0_1|\n",
            "|    2|1.0|3.0|  1.0_1|\n",
            "|    3|1.0|4.0|  1.0_1|\n",
            "|    3|1.0|4.0|  1.0_1|\n",
            "+-----+---+---+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp1C5_dxQ84z",
        "colab_type": "code",
        "outputId": "7d88f0e8-69f0-44ee-d5b7-4b693cc2735f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b.select(concat(b['num'].cast(StringType()),lit(\"_1\"))  ).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------+\n",
            "|concat(CAST(num AS STRING), _1)|\n",
            "+-------------------------------+\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "+-------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rizTE6jTJyE",
        "colab_type": "code",
        "outputId": "72c24fa5-4b30-44d4-87c6-e9f7278c70e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b.withColumn(\"numToString\",concat(b['num'].cast(StringType()),lit(\"_1\"))  ).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+-----------+\n",
            "|class|num|new|numToString|\n",
            "+-----+---+---+-----------+\n",
            "|    0|1.0|1.0|      1.0_1|\n",
            "|    0|1.0|1.0|      1.0_1|\n",
            "|    1|1.0|2.0|      1.0_1|\n",
            "|    1|1.0|2.0|      1.0_1|\n",
            "|    2|1.0|3.0|      1.0_1|\n",
            "|    2|1.0|3.0|      1.0_1|\n",
            "|    3|1.0|4.0|      1.0_1|\n",
            "|    3|1.0|4.0|      1.0_1|\n",
            "+-----+---+---+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqqu2w-4n9_k",
        "colab_type": "text"
      },
      "source": [
        "Let us move on for grouped_map ; hope to see the use of scalar map in future.\n",
        "For now, I am assuming that the time taken for the same is different and pandas function must be faster than sql functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEv_Oiph_kwI",
        "colab_type": "code",
        "outputId": "0bb65854-7fb9-4147-fccf-b729181936f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "%%timeit\n",
        "b.select( numToString(col('num'))).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|num_to_string(num)|\n",
            "+------------------+\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "+------------------+\n",
            "\n",
            "+------------------+\n",
            "|num_to_string(num)|\n",
            "+------------------+\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "+------------------+\n",
            "\n",
            "+------------------+\n",
            "|num_to_string(num)|\n",
            "+------------------+\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "+------------------+\n",
            "\n",
            "+------------------+\n",
            "|num_to_string(num)|\n",
            "+------------------+\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "|             1.0_1|\n",
            "+------------------+\n",
            "\n",
            "1 loop, best of 3: 329 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enEXVtUwpacl",
        "colab_type": "code",
        "outputId": "9cdd8315-9792-4040-b10d-04c71bc758b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "%%timeit\n",
        "b.select(concat(b['num'].cast(StringType()),lit(\"_1\"))  ).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------+\n",
            "|concat(CAST(num AS STRING), _1)|\n",
            "+-------------------------------+\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "+-------------------------------+\n",
            "\n",
            "+-------------------------------+\n",
            "|concat(CAST(num AS STRING), _1)|\n",
            "+-------------------------------+\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "+-------------------------------+\n",
            "\n",
            "+-------------------------------+\n",
            "|concat(CAST(num AS STRING), _1)|\n",
            "+-------------------------------+\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "+-------------------------------+\n",
            "\n",
            "+-------------------------------+\n",
            "|concat(CAST(num AS STRING), _1)|\n",
            "+-------------------------------+\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "|                          1.0_1|\n",
            "+-------------------------------+\n",
            "\n",
            "1 loop, best of 3: 248 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlYoFRhKp7x_",
        "colab_type": "text"
      },
      "source": [
        "The above experiment do not conclude that either of the functions( pandas or sql) is faster. In order to conclude that we need not only a huge data set but also multiple runs to conclude the same.\n",
        "\n",
        "Hence, I will continue to assume there may be a difference in performance when it comes to 2 functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyi6gGJXqyHo",
        "colab_type": "text"
      },
      "source": [
        "**GROUPED_MAP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuvqybfRpp-u",
        "colab_type": "code",
        "outputId": "c134bab1-571f-416b-b3bf-fe71f807b05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "b.groupBy(col('class')).apply(lambda x: x).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e3bbf63c15cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, udf)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'func'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m            \u001b[0;32mor\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalType\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPythonEvalType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSQL_GROUPED_MAP_PANDAS_UDF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             raise ValueError(\"Invalid udf: the udf argument must be a pandas_udf of type \"\n\u001b[0m\u001b[1;32m    272\u001b[0m                              \"GROUPED_MAP.\")\n\u001b[1;32m    273\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhKRyw2gt-sq",
        "colab_type": "text"
      },
      "source": [
        "### Note: Pandas DataFrame.assign not only creates new column but also creates a copy of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XiKMkqsslQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "@pandas_udf(\"class long, num double, new long\", PandasUDFType.GROUPED_MAP)\n",
        "def subtract_mean(pdf):\n",
        "  \n",
        "  return pdf.assign(num=pdf.num*2)\n",
        "\n",
        "@pandas_udf(\"class long, num double\", PandasUDFType.GROUPED_MAP)\n",
        "def mutltipl_3(pdf):\n",
        "  return pdf.assign(num=pdf.num*3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSGB0R0awHEd",
        "colab_type": "code",
        "outputId": "43a10ecc-985a-4727-87a9-2465af83105b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b.groupBy(\"num\").apply(subtract_mean).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+\n",
            "|class|num|new|\n",
            "+-----+---+---+\n",
            "|    0|2.0|  1|\n",
            "|    0|2.0|  1|\n",
            "|    1|2.0|  2|\n",
            "|    1|2.0|  2|\n",
            "|    2|2.0|  3|\n",
            "|    2|2.0|  3|\n",
            "|    3|2.0|  4|\n",
            "|    3|2.0|  4|\n",
            "+-----+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGFthjT4HhzS",
        "colab_type": "code",
        "outputId": "0773bf19-5220-4152-eedc-2458bf9d781e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2434
        }
      },
      "source": [
        "b.groupBy(\"num\").apply(mutltipl_3).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-91fb22094b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmutltipl_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o223.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 33 in stage 51.0 failed 1 times, most recent failure: Lost task 33.0 in stage 51.0 (TID 362, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 286, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 125, in wrapped\n    \"Expected: {} Actual: {}\".format(len(return_type), len(result.columns)))\nRuntimeError: Number of columns of the returned pandas.DataFrame doesn't match specified schema. Expected: 2 Actual: 3\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 286, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 125, in wrapped\n    \"Expected: {} Actual: {}\".format(len(return_type), len(result.columns)))\nRuntimeError: Number of columns of the returned pandas.DataFrame doesn't match specified schema. Expected: 2 Actual: 3\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd_mQhM_wd7R",
        "colab_type": "code",
        "outputId": "4e041668-728b-449b-bc3d-1b7f4506cc5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "b.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+\n",
            "|class|num|new|\n",
            "+-----+---+---+\n",
            "|    0|1.0|1.0|\n",
            "|    0|1.0|1.0|\n",
            "|    1|1.0|2.0|\n",
            "|    1|1.0|2.0|\n",
            "|    2|1.0|3.0|\n",
            "|    2|1.0|3.0|\n",
            "|    3|1.0|4.0|\n",
            "|    3|1.0|4.0|\n",
            "+-----+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LY7HiHWEr9V",
        "colab_type": "text"
      },
      "source": [
        "So it is not only works on the multiple columns returning same shape  but also works split-apply-combine functions.\n",
        "\n",
        "Splits based on groupby clause and can cause overflow if the dataset for the id selected in groupby is skewed.\n",
        "\n",
        "Again here these operation can be done parallelly by sql functions too but moving towards data science architecture like it in panda syntax rather to sql syntax.\n",
        "\n",
        "Also it is important to note that we ***need***  give all the columns in the grouped map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW78J5-NI1Hc",
        "colab_type": "text"
      },
      "source": [
        "#### GROUPED_AGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHS2w8Nkx0ct",
        "colab_type": "code",
        "outputId": "1c77a6c3-481f-4835-81de-b33577f83b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
        "def mean_udf(v):\n",
        "    return v.mean()\n",
        "  \n",
        "b.groupBy(\"class\").agg(mean_udf('new')).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-------------+\n",
            "|class|mean_udf(new)|\n",
            "+-----+-------------+\n",
            "|    0|          1.0|\n",
            "|    1|          2.0|\n",
            "|    3|          4.0|\n",
            "|    2|          3.0|\n",
            "+-----+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN8RlFy_LE5y",
        "colab_type": "text"
      },
      "source": [
        "#### Last but not the least, the SQL queries on dataframe for SQL lovers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thhuS4k9IuXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b.createOrReplaceTempView(\"TempTable\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhxIsRQOLSyU",
        "colab_type": "code",
        "outputId": "f4e68490-530b-48b6-de59-c6e5dcf76d23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "spark.sql(\"select * from TempTable\").show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+\n",
            "|class|num|new|\n",
            "+-----+---+---+\n",
            "|    0|1.0|1.0|\n",
            "|    0|1.0|1.0|\n",
            "|    1|1.0|2.0|\n",
            "|    1|1.0|2.0|\n",
            "|    2|1.0|3.0|\n",
            "|    2|1.0|3.0|\n",
            "|    3|1.0|4.0|\n",
            "|    3|1.0|4.0|\n",
            "+-----+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjw0V9WyMC2s",
        "colab_type": "code",
        "outputId": "cbdec3e0-3170-4b74-8788-28ef405825d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "spark.sql(\"select class, avg(new) from TempTable group by class\").show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------+\n",
            "|class|avg(new)|\n",
            "+-----+--------+\n",
            "|    0|     1.0|\n",
            "|    1|     2.0|\n",
            "|    3|     4.0|\n",
            "|    2|     3.0|\n",
            "+-----+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxcAIFJfSg4t",
        "colab_type": "text"
      },
      "source": [
        "### ETL\n",
        "https://medium.com/@mrpowers/how-to-write-spark-etl-processes-df01b0c1bec9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB8XJFSKLutK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import expanduser, join, abspath\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# warehouse_location points to the default location for managed databases and tables\n",
        "warehouse_location = abspath('spark-warehouse')\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKILcHfEcDOe",
        "colab_type": "code",
        "outputId": "b80695a5-96ca-4a48-b499-42a9f47d0d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "source": [
        "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);;\n'CreateTable `src`, Ignore\n\n\tat org.apache.spark.sql.execution.datasources.HiveOnlyCheck$$anonfun$apply$12.apply(rules.scala:392)\n\tat org.apache.spark.sql.execution.datasources.HiveOnlyCheck$$anonfun$apply$12.apply(rules.scala:390)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)\n\tat org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:390)\n\tat org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:388)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$2.apply(CheckAnalysis.scala:386)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$2.apply(CheckAnalysis.scala:386)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:386)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a432af1d3fec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: \"Hive support is required to CREATE Hive TABLE (AS SELECT);;\\n'CreateTable `src`, Ignore\\n\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9325SCQeNx5",
        "colab_type": "text"
      },
      "source": [
        "#### Since we do not hve support, we cannot execute here. but is it any use?\n",
        "\n",
        "https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html\n",
        "\n",
        "\n",
        "df.write.partitionBy(\"key\").format(\"hive\").saveAsTable(\"hive_part_tbl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L87TshZmJTH",
        "colab_type": "text"
      },
      "source": [
        "#### Deep Learning at Spark\n",
        "* https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/spark/mnist_spark.py\n",
        "\n",
        "* https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector#usage-examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a8KYDzBwHSf",
        "colab_type": "text"
      },
      "source": [
        "##### Worked on a Stackoverflow query:\n",
        "https://stackoverflow.com/a/56389834/8693106"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ZZhaK2hS6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c3322fb4-9f4a-4322-ea06-3cf3f214f194"
      },
      "source": [
        "data = [((\"ID1\", 3, 5,78)), ((\"ID2\", 4, 12,45)), ((\"ID3\", 68, 3,67))]\n",
        "df = spark.createDataFrame(data, [\"ID\", \"colA\", \"colB\",\"colC\"])\n",
        "df.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+----+----+\n",
            "| ID|colA|colB|colC|\n",
            "+---+----+----+----+\n",
            "|ID1|   3|   5|  78|\n",
            "|ID2|   4|  12|  45|\n",
            "|ID3|  68|   3|  67|\n",
            "+---+----+----+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of1RkJJvKgm7",
        "colab_type": "text"
      },
      "source": [
        "##### How to append a column in pyspark dataframe with a literal value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO1pGyONzumH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ced34c53-03ae-4c40-b2e9-282a2ad38dae"
      },
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df= df.withColumn('Max_col',lit(\"colA\"))\n",
        "df.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+----+----+-------+\n",
            "| ID|colA|colB|colC|Max_col|\n",
            "+---+----+----+----+-------+\n",
            "|ID1|   3|   5|  78|   colA|\n",
            "|ID2|   4|  12|  45|   colA|\n",
            "|ID3|   8|   3|  67|   colA|\n",
            "+---+----+----+----+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaulW3d0Kol5",
        "colab_type": "text"
      },
      "source": [
        "##### How to drop a column in pyspark dataframe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyT4ExpICRyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "77efc8c3-bb3a-43a4-b1ff-c4502c3fcb86"
      },
      "source": [
        "df =df.drop('Max_col')\n",
        "df.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+----+----+\n",
            "| ID|colA|colB|colC|\n",
            "+---+----+----+----+\n",
            "|ID1|   3|   5|  78|\n",
            "|ID2|   4|  12|  45|\n",
            "|ID3|   8|   3|  67|\n",
            "+---+----+----+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA3eaxlHK4z-",
        "colab_type": "text"
      },
      "source": [
        "### UDF and Not Pandas_udf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7BJ7QVE0usJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4a5e036b-6aae-4b62-cfad-41edc574c388"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.types import IntegerType, StringType\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data = [((\"ID1\", 3, 5,78)), ((\"ID2\", 4, 12,45)), ((\"ID3\", 68, 3,67))]\n",
        "df = spark.createDataFrame(data, [\"ID\", \"colA\", \"colB\",\"colC\"])\n",
        "df.show()\n",
        "\n",
        "cols = df.columns\n",
        "#maxcol = f.udf(lambda row: cols[row.index(max(row)) +1], StringType())\n",
        "maxcol = f.udf(lambda row: int(np.argmax(row)), IntegerType())\n",
        "\n",
        "maxDF = df.withColumn(\"maxval\", maxcol(f.struct([df[x] for x in df.columns[1:]])))\n",
        "maxDF.show(truncate=False)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+----+----+\n",
            "| ID|colA|colB|colC|\n",
            "+---+----+----+----+\n",
            "|ID1|   3|   5|  78|\n",
            "|ID2|   4|  12|  45|\n",
            "|ID3|  68|   3|  67|\n",
            "+---+----+----+----+\n",
            "\n",
            "+---+----+----+----+------+\n",
            "|ID |colA|colB|colC|maxval|\n",
            "+---+----+----+----+------+\n",
            "|ID1|3   |5   |78  |2     |\n",
            "|ID2|4   |12  |45  |2     |\n",
            "|ID3|68  |3   |67  |0     |\n",
            "+---+----+----+----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5symca_KBRTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "efefbe3f-ffa3-4562-ddae-2da6e1841b1c"
      },
      "source": [
        "np.argmax()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_JU82HuC6oo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b520c4e-9957-4f59-fbf2-bc143c4d696b"
      },
      "source": [
        "cols.index('colB')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC8c5ks9C_Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}