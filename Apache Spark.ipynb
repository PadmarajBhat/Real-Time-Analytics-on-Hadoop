{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction as in Apache Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import S\n",
    "\n",
    "sc = SparkContext(appName=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter == Query in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile[textFile.value.contains(\"abc\")].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note that the above case has created RDD and dataframe because the file has been read using spark context and not SQL context\n",
    "\n",
    "Let us create sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "spark = SQLContext(sc).sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.read.text(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[text.value.contains(\"get\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating new column in spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text.select(size(split(text.value,\"\\s+\")).name(\"wordCount\"))\n",
    "text1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.select(size(split(text.value,\"\\s+\")).name(\"wordCount\")).agg(max(col(\"wordCount\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulating MapReduce action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.select(split(text.value,\"\\s+\").name(\"word\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " text.select(split(text.value,\"\\s+\").name(\"word\")).groupby(\"word\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explode, GroupBy and OrderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = text.select(explode(split(text.value,\"\\S +\")).name(\"word\")).groupby(\"word\").count().orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importance of Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One more way of creating spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark2 = SparkSession.builder.appName(\"temp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2.read.text(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Note on Cluster and spark program execution on them\n",
    "* one single python spark program has 2 logical components.\n",
    "    * Driver tasks\n",
    "        * To assgin a task and collect the task operation output.\n",
    "        * It cordianates with resource managers like yarn and mesos for resource availing. This is transperant to the programmer as he does only read, write and dataframe related operations. \n",
    "    * Worker tasks\n",
    "        * indicates its presence to resource allocator and hence gets nominated for a task\n",
    "        * executes the instructions as per driver program\n",
    "        * note that here operation is limited to the data available in its scope and hence execution is faster and also parallel processing is feasible.\n",
    "        \n",
    "        \n",
    "* Monitoring can be done through: http://<driver-node>:4040\n",
    "* There are application like livy which also provides interactive access to jobs and thier features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Job Scheduling\n",
    "* unlike, in any other scheduling context, here we are talking about the resource scheduling\n",
    "\n",
    "\n",
    "* Static Partitioning:\n",
    "    * number of resources to a job can be limited through spark submit arguments\n",
    "        * --num-executors : number of executors for a job\n",
    "        * --executor-memory\n",
    "        * --executor-cores : Note cores are logical units of computation in CPU\n",
    "\n",
    "\n",
    "\n",
    "* Dynamic Resource Allocation:\n",
    "    * Jobs can return the resource after current usage and request again for later use.\n",
    "    * set spark.dynamicAllocation.enabled to true\n",
    "    * set spark.shuffle.service.enabled to true\n",
    "        * enabling resource executor to be removed but not the shuffle files. So intermediate results are retained so that when executor requires it, can be fetched. \n",
    "        * Shuffle service which always run and can collect all the shuffle files info across application can avoid a scenario of new executor trying to access the old executor shuffule file in progress writing.\n",
    "            * Here Old executor can submit a shuffle right to the shuffle services and terminate gracefully.\n",
    "            * Shuffle service will handle the situation of new executor requesting the old shuffle file content.\n",
    "\n",
    "    * spark.dynamicAllocation.schedulerBacklogTimeout is used to trigger the request.\n",
    "        * if not availed per first request, it will request again for every spark.dynamicAllocation.sustainedSchedulerBacklogTimeout seconds. \n",
    "        * executor count is increased exponentially for every subsequent request\n",
    "            * this is because job was waiting for it for so long and has to catch up for the waiting period\n",
    "            * it also acts as a buffer for future use (as the intial request did not get fulfill).\n",
    "            \n",
    "    * executors are removed after spark.dynamicAllocation.executorIdleTimeout seconds.\n",
    "            \n",
    "        \n",
    "        \n",
    "* Scheduling Within an Application:\n",
    "    * Fair scheduling: all jobs within application like save, collect etc are by default executed FIFO manner but Fair scheduling can allocate resources to smaller jobs even wen long jobs are being execute.\n",
    "        * conf.set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "        * Pools can also have \n",
    "            * weights to decide which has to be given higher preference\n",
    "            * minShare to decide which is the minimum share despite being lower weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shared Variables\n",
    "* Broadcast Variables:\n",
    "    * a variable is broadcasted to all the nodes in the cluster.\n",
    "    * read only copy\n",
    "    * used when repeated usage of large dataset.\n",
    "        * broadcastVar = sc.broadcast([1, 2, 3])\n",
    "        * broadcastVar.value\n",
    "\n",
    "* Accumulator Variables:\n",
    "    * associative or cumulative operations on a varible by different tasks\n",
    "    * cluster nodes can add values through (+=) operator to it but cannot read\n",
    "    * only driver program can read it.\n",
    "    \n",
    "* Spark Streaming:\n",
    "    * uses DStream API using Spark RDD support\n",
    "    * input stream is divided into micro batch and executed\n",
    "    * each micro batch is a RDD\n",
    "    * after each micror batch source is polled for new micro batch\n",
    "    * foreachRDD fetches you the data of each micro batch\n",
    "   \n",
    "* Spark Structured Streaming:\n",
    "    * built on top of spark SQL programming, leverages the dataframe apis\n",
    "    * new data input is row in unbound table\n",
    "    * can handle late event data\n",
    "    * foreachBatch gives resultant dataframe \n",
    "    * here latency is 100 ms\n",
    "        * Contnuous processing (>2.3): has the end to end latency of 1ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of Spark Structured Streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\",\"9999\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = wordCount.writeStream.format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
