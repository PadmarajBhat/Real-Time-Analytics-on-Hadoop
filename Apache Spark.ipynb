{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction as in Apache Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import S\n",
    "\n",
    "sc = SparkContext(appName=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter == Query in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile[textFile.value.contains(\"abc\")].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note that the above case has created RDD and dataframe because the file has been read using spark context and not SQL context\n",
    "\n",
    "Let us create sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "spark = SQLContext(sc).sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.read.text(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[text.value.contains(\"get\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating new column in spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text.select(size(split(text.value,\"\\s+\")).name(\"wordCount\"))\n",
    "text1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.select(size(split(text.value,\"\\s+\")).name(\"wordCount\")).agg(max(col(\"wordCount\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulating MapReduce action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.select(split(text.value,\"\\s+\").name(\"word\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " text.select(split(text.value,\"\\s+\").name(\"word\")).groupby(\"word\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explode, GroupBy and OrderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = text.select(explode(split(text.value,\"\\S +\")).name(\"word\")).groupby(\"word\").count().orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importance of Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One more way of creating spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark2 = SparkSession.builder.appName(\"temp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2.read.text(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Note on Cluster and spark program execution on them\n",
    "* one single python spark program has 2 logical components.\n",
    "    * Driver tasks\n",
    "        * To assgin a task and collect the task operation output.\n",
    "        * It cordianates with resource managers like yarn and mesos for resource availing. This is transperant to the programmer as he does only read, write and dataframe related operations. \n",
    "    * Worker tasks\n",
    "        * indicates its presence to resource allocator and hence gets nominated for a task\n",
    "        * executes the instructions as per driver program\n",
    "        * note that here operation is limited to the data available in its scope and hence execution is faster and also parallel processing is feasible.\n",
    "        \n",
    "        \n",
    "* Monitoring can be done through: http://<driver-node>:4040\n",
    "* There are application like livy which also provides interactive access to jobs and thier features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Job Scheduling\n",
    "* unlike, in any other scheduling context, here we are talking about the resource scheduling\n",
    "\n",
    "\n",
    "* Static Partitioning:\n",
    "    * number of resources to a job can be limited through spark submit arguments\n",
    "        * --num-executors : number of executors for a job\n",
    "        * --executor-memory\n",
    "        * --executor-cores : Note cores are logical units of computation in CPU\n",
    "\n",
    "\n",
    "\n",
    "* Dynamic Resource Allocation:\n",
    "    * Jobs can return the resource after current usage and request again for later use.\n",
    "    * set spark.dynamicAllocation.enabled to true\n",
    "    * set spark.shuffle.service.enabled to true\n",
    "        * enabling resource executor to be removed but not the shuffle files. So intermediate results are retained so that when executor requires it, can be fetched. \n",
    "        * Shuffle service which always run and can collect all the shuffle files info across application can avoid a scenario of new executor trying to access the old executor shuffule file in progress writing.\n",
    "            * Here Old executor can submit a shuffle right to the shuffle services and terminate gracefully.\n",
    "            * Shuffle service will handle the situation of new executor requesting the old shuffle file content.\n",
    "\n",
    "    * spark.dynamicAllocation.schedulerBacklogTimeout is used to trigger the request.\n",
    "        * if not availed per first request, it will request again for every spark.dynamicAllocation.sustainedSchedulerBacklogTimeout seconds. \n",
    "        * executor count is increased exponentially for every subsequent request\n",
    "            * this is because job was waiting for it for so long and has to catch up for the waiting period\n",
    "            * it also acts as a buffer for future use (as the intial request did not get fulfill).\n",
    "            \n",
    "    * executors are removed after spark.dynamicAllocation.executorIdleTimeout seconds.\n",
    "            \n",
    "        \n",
    "        \n",
    "* Scheduling Within an Application:\n",
    "    * Fair scheduling: all jobs within application like save, collect etc are by default executed FIFO manner but Fair scheduling can allocate resources to smaller jobs even wen long jobs are being execute.\n",
    "        * conf.set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "        * Pools can also have \n",
    "            * weights to decide which has to be given higher preference\n",
    "            * minShare to decide which is the minimum share despite being lower weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shared Variables\n",
    "* Broadcast Variables:\n",
    "    * a variable is broadcasted to all the nodes in the cluster.\n",
    "    * read only copy\n",
    "    * used when repeated usage of large dataset.\n",
    "        * broadcastVar = sc.broadcast([1, 2, 3])\n",
    "        * broadcastVar.value\n",
    "\n",
    "* Accumulator Variables:\n",
    "    * associative or cumulative operations on a varible by different tasks\n",
    "    * cluster nodes can add values through (+=) operator to it but cannot read\n",
    "    * only driver program can read it.\n",
    "    \n",
    "* Spark Streaming:\n",
    "    * uses DStream API using Spark RDD support\n",
    "    * input stream is divided into micro batch and executed\n",
    "    * each micro batch is a RDD\n",
    "    * after each micror batch source is polled for new micro batch\n",
    "    * foreachRDD fetches you the data of each micro batch\n",
    "   \n",
    "* Spark Structured Streaming:\n",
    "    * built on top of spark SQL programming, leverages the dataframe apis\n",
    "    * new data input is row in unbound table\n",
    "    * can handle late event data\n",
    "    * foreachBatch gives resultant dataframe \n",
    "    * here latency is 100 ms\n",
    "        * Contnuous processing (>2.3): has the end to end latency of 1ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of Spark Structured Streaming:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output mode: Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\",\"9999\").load()\n",
    "\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "wordCount = words.groupBy(\"word\").count()\n",
    "\n",
    "query = wordCount.writeStream.format(\"console\").outputMode(\"complete\").trigger(processingTime='2 seconds').start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "userSchema = StructType()\\\n",
    "\t\t\t\t.add(\"mp_constituency\", \"string\")\\\n",
    "\t\t\t\t.add(\"assembly\", \"string\")\\\n",
    "\t\t\t\t.add(\"ward_no\", \"string\")\\\n",
    "\t\t\t\t.add(\"ward_name\", \"string\")\\\n",
    "\t\t\t\t.add(\"winning_candidate\", \"string\")  \\\n",
    "\t\t\t\t.add(\"winners_party\", \"string\")  \\\n",
    "\t\t\t\t.add(\"past_corporator_party\", \"string\") \n",
    "\n",
    "csvDF = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .schema(userSchema) \\\n",
    "    .csv(r\"C:\\Users\\Dell\\Desktop\\Hadoop\\Real Time Analytics with Apache Storm\\csvFolder\")\n",
    "\n",
    "partySeats = csvDF.groupBy(\"winners_party\").count()\n",
    "query = partySeats.writeStream.format(\"console\").outputMode(\"complete\").trigger(processingTime='2 seconds').start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that above 2 cells are executed at anaconda command prompt\n",
    "\n",
    "* python sparkInputFile.py\n",
    "* spark-submit pysparkInputFile.py > sparkOutput\n",
    "    * here spark output will be on terminal\n",
    "    * however, sparkOutput will have application output [writeStream console output]\n",
    "        * each time a new file is being processed update to result table is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "userSchema could have been also through:\n",
    "\n",
    "```\n",
    "name_list = [\"mp_constituency\", \"assembly\", \"ward_no\", \"ward_name\", \"winning_candidate\", \"winners_party\", \"past_corporator_party\"]\n",
    "userSchema = StructType([StructField(field_name, StringType(), True) for field_name in name_list])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output mode Append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "'''\n",
    "userSchema = StructType()\\\n",
    "\t\t\t\t.add(\"mp_constituency\", \"string\")\\\n",
    "\t\t\t\t.add(\"assembly\", \"string\")\\\n",
    "\t\t\t\t.add(\"ward_no\", \"string\")\\\n",
    "\t\t\t\t.add(\"ward_name\", \"string\")\\\n",
    "\t\t\t\t.add(\"winning_candidate\", \"string\")  \\\n",
    "\t\t\t\t.add(\"winners_party\", \"string\")  \\\n",
    "\t\t\t\t.add(\"past_corporator_party\", \"string\") \n",
    "'''\n",
    "name_list = [\"mp_constituency\", \"assembly\", \"ward_no\", \"ward_name\", \"winning_candidate\", \"winners_party\", \"past_corporator_party\"]\n",
    "userSchema = StructType([StructField(field_name, StringType(), True) for field_name in name_list])\n",
    "\n",
    "csvDF = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(userSchema) \\\n",
    "    .csv(r\"C:\\Users\\Dell\\Desktop\\Hadoop\\Real Time Analytics with Apache Storm\\csvFolder\")\n",
    "\n",
    "\n",
    "isManjula = csvDF[csvDF.winning_candidate.contains(\"Manjula\")]\n",
    "query = isManjula.writeStream.format(\"console\").outputMode(\"append\").trigger(processingTime='2 seconds').start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output for the above pgm\n",
    "```\n",
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "|  mp_constituency|           assembly|ward_no|  ward_name|   winning_candidate|winners_party|past_corporator_party|\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "|  Bangalore North|Rajarajeswari Nagar|     69|    Laggere|Manjula Narayanas...|          JDS|                  BJP|\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "|  Bangalore North|Rajarajeswari Nagar|     69|    Laggere|Manjula Narayanas...|          JDS|                  BJP|\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "|  Bangalore North|Rajarajeswari Nagar|     69|    Laggere|Manjula Narayanas...|          JDS|                  BJP|\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "|  Bangalore North|Rajarajeswari Nagar|     69|    Laggere|Manjula Narayanas...|          JDS|                  BJP|\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "|  Bangalore North|Rajarajeswari Nagar|     69|    Laggere|Manjula Narayanas...|          JDS|                  BJP|\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "|  Bangalore North|Rajarajeswari Nagar|     69|    Laggere|Manjula Narayanas...|          JDS|                  BJP|\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "19/05/10 21:26:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 5689 milliseconds\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "|  mp_constituency|           assembly|ward_no|  ward_name|   winning_candidate|winners_party|past_corporator_party|\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "|  Bangalore North|Rajarajeswari Nagar|     69|    Laggere|Manjula Narayanas...|          JDS|                  BJP|\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "\n",
    "19/05/10 21:26:33 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3666 milliseconds\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "|  mp_constituency|           assembly|ward_no|  ward_name|   winning_candidate|winners_party|past_corporator_party|\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "|  Bangalore South|      B.T.M. Layout|    147|    Adugodi|             Manjula|          INC|                  INC|\n",
    "|Bangalore Central|       Rajaji Nagar|    107|Shiva Nagar|  Manjula Vijaykumar|          INC|                  BJP|\n",
    "|  Bangalore North|Rajarajeswari Nagar|     69|    Laggere|Manjula Narayanas...|          JDS|                  BJP|\n",
    "+-----------------+-------------------+-------+-----------+--------------------+-------------+---------------------+\n",
    "\n",
    "19/05/10 21:27:07 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 7877 milliseconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here each bath is new file being added to directory or files recognized in the intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output mode \"Update\"\n",
    "* With the simple change to the earlier program we can see the \"update\" output.\n",
    "\n",
    "```\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "'''\n",
    "userSchema = StructType()\\\n",
    "\t\t\t\t.add(\"mp_constituency\", \"string\")\\\n",
    "\t\t\t\t.add(\"assembly\", \"string\")\\\n",
    "\t\t\t\t.add(\"ward_no\", \"string\")\\\n",
    "\t\t\t\t.add(\"ward_name\", \"string\")\\\n",
    "\t\t\t\t.add(\"winning_candidate\", \"string\")  \\\n",
    "\t\t\t\t.add(\"winners_party\", \"string\")  \\\n",
    "\t\t\t\t.add(\"past_corporator_party\", \"string\") \n",
    "'''\n",
    "name_list = [\"mp_constituency\", \"assembly\", \"ward_no\", \"ward_name\", \"winning_candidate\", \"winners_party\", \"past_corporator_party\"]\n",
    "userSchema = StructType([StructField(field_name, StringType(), True) for field_name in name_list])\n",
    "\n",
    "csvDF = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(userSchema) \\\n",
    "    .csv(r\"C:\\Users\\Dell\\Desktop\\Hadoop\\Real Time Analytics with Apache Storm\\csvFolder\")\n",
    "\n",
    "partySeats = csvDF.groupBy(\"winners_party\").count()\n",
    "query = partySeats.writeStream.format(\"console\").outputMode(\"update\").trigger(processingTime='2 seconds').start()\n",
    "#isManjula = csvDF[csvDF.winning_candidate.contains(\"Manjula\")]\n",
    "#query = isManjula.writeStream.format(\"console\").outputMode(\"update\").trigger(processingTime='2 seconds').start()\n",
    "\n",
    "\n",
    "query.awaitTermination()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output for the above program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(base) C:\\Users\\Dell\\Desktop\\Hadoop\\Real Time Analytics with Apache Storm>python StucturedStreamingOutputAppend.py\n",
    "19/05/10 21:47:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "19/05/10 21:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "19/05/10 21:47:46 WARN SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes\n",
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+-------------+-----+\n",
    "|winners_party|count|\n",
    "+-------------+-----+\n",
    "|         SDPI|    9|\n",
    "|          INC|  684|\n",
    "|          BJP|  900|\n",
    "|          JDS|  126|\n",
    "|       Others|   27|\n",
    "|  Independent|   36|\n",
    "+-------------+-----+\n",
    "\n",
    "19/05/10 21:50:43 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 172900 milliseconds\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+-------------+-----+\n",
    "|winners_party|count|\n",
    "+-------------+-----+\n",
    "|          BJP|  979|\n",
    "|          JDS|  137|\n",
    "|       Others|   29|\n",
    "|  Independent|   37|\n",
    "+-------------+-----+\n",
    "\n",
    "19/05/10 21:54:09 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 187771 milliseconds\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+-------------+-----+\n",
    "|winners_party|count|\n",
    "+-------------+-----+\n",
    "|         SDPI|   10|\n",
    "|          INC|  760|\n",
    "|          BJP| 1000|\n",
    "|          JDS|  140|\n",
    "|       Others|   30|\n",
    "|  Independent|   40|\n",
    "+-------------+-----+\n",
    "\n",
    "19/05/10 21:57:01 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 141160 milliseconds\n",
    "-------------------------------------------\n",
    "Batch: 3\n",
    "-------------------------------------------\n",
    "+-------------+-----+\n",
    "|winners_party|count|\n",
    "+-------------+-----+\n",
    "|          INC|  836|\n",
    "+-------------+-----+\n",
    "\n",
    "19/05/10 22:01:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 140606 milliseconds\n",
    "\n",
    "```\n",
    "\n",
    "* in batch 1, I removed INC\n",
    "* in batch 3, I added only INC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map & Faltmap are no longer in use for dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"sparkOutput\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='-------------------------------------------'),\n",
       " Row(value='Batch: 0'),\n",
       " Row(value='-------------------------------------------'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value='|winners_party|count|'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value='|         SDPI|    7|'),\n",
       " Row(value='|          INC|  532|'),\n",
       " Row(value='|          BJP|  700|'),\n",
       " Row(value='|          JDS|   98|'),\n",
       " Row(value='|       Others|   21|'),\n",
       " Row(value='|  Independent|   28|'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value=''),\n",
       " Row(value='-------------------------------------------'),\n",
       " Row(value='Batch: 1'),\n",
       " Row(value='-------------------------------------------'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value='|winners_party|count|'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value='|         SDPI|    8|'),\n",
       " Row(value='|          INC|  608|'),\n",
       " Row(value='|          BJP|  800|'),\n",
       " Row(value='|          JDS|  112|'),\n",
       " Row(value='|       Others|   24|'),\n",
       " Row(value='|  Independent|   32|'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value=''),\n",
       " Row(value='-------------------------------------------'),\n",
       " Row(value='Batch: 2'),\n",
       " Row(value='-------------------------------------------'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value='|winners_party|count|'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value='|         SDPI|    9|'),\n",
       " Row(value='|          INC|  684|'),\n",
       " Row(value='|          BJP|  900|'),\n",
       " Row(value='|          JDS|  126|'),\n",
       " Row(value='|       Others|   27|'),\n",
       " Row(value='|  Independent|   36|'),\n",
       " Row(value='+-------------+-----+'),\n",
       " Row(value=''),\n",
       " Row(value='Terminate batch job (Y/N)? '),\n",
       " Row(value='Terminate batch job (Y/N)? Terminate batch job (Y/N)? ')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(split(lines.value,\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(explode(split(lines.value,\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(explode(split(lines.value,\" \")).name(\"word\")).groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                 27||    1|\n",
      "|                  7||    1|\n",
      "|           Terminate|    3|\n",
      "|                  8||    1|\n",
      "|-----------------...|    6|\n",
      "|               SDPI||    3|\n",
      "|                  9||    1|\n",
      "|                   0|    1|\n",
      "|+-------------+--...|    9|\n",
      "|                 36||    1|\n",
      "|                JDS||    3|\n",
      "|                 21||    1|\n",
      "|                 job|    3|\n",
      "|                112||    1|\n",
      "|                800||    1|\n",
      "|                684||    1|\n",
      "|                   ||   18|\n",
      "|               batch|    3|\n",
      "|             Others||    3|\n",
      "||winners_party|co...|    3|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that if the file gets updated then spark donot selects the new rows. How do we manage the file updates ? How to avoid dirty read ? or How to have committed read ?\n",
    "\n",
    "* It is clearly mentioned in the doc that\n",
    "\"Once processed, changes to a file within the current window will not cause the file to be reread. That is: updates are ignored.\" at https://spark.apache.org/docs/latest/streaming-programming-guide.html#basic-sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Window operations (details are best described at https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }\n",
    "\n",
    "# Group the data by window and word and compute the count of each group\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n",
    "    words.word\n",
    ").count()\n",
    "```\n",
    "\n",
    "Here window attribute is on the field by name timestam, window width is 10 minutes. Sliding happens for 5 hopes.\n",
    "* 5-15 , 10-20 etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling late event data through water marking\n",
    "* watermarking, which lets the engine automatically track the current event time in the data and attempt to clean up old state accordingly. \n",
    "\n",
    "```\n",
    "words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }\n",
    "\n",
    "# Group the data by window and word and compute the count of each group\n",
    "windowedCounts = words \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n",
    "        words.word) \\\n",
    "    .count()\n",
    "```\n",
    "* here withWatermark defines how long spark should wait for the late entry of a data. Here spark would wait for 10 minutes and beyond that limit it would ignore the data for any aggregation or selection operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for Window operation\n",
    "```\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "userSchema = StructType()\\\n",
    "\t\t\t\t.add(\"task\", \"string\")\\\n",
    "\t\t\t\t.add(\"time\", \"string\") \n",
    "\n",
    "#name_list = [\"mp_constituency\", \"assembly\", \"ward_no\", \"ward_name\", \"winning_candidate\", \"winners_party\", \"past_corporator_party\"]\n",
    "#userSchema = StructType([StructField(field_name, StringType(), True) for field_name in name_list])\n",
    "\n",
    "csvDF = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .schema(userSchema) \\\n",
    "    .csv(r\"C:\\Users\\Dell\\Desktop\\Hadoop\\Real Time Analytics with Apache Storm\\time\")\n",
    "\n",
    "\n",
    "csvDF = csvDF.withColumn(\"timestamp\",to_timestamp(\"time\",\"dd-mm-yyyy hh:mm\"))\n",
    "\n",
    "partySeats = csvDF.groupBy(\"timestamp\").count()\n",
    "\n",
    "query = partySeats.writeStream.format(\"console\").outputMode(\"complete\").start()\n",
    "\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "* The above program would run as a normal spark structured stream; focus is on timestamp dataset creation and conversion to spark sql timestamp format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
