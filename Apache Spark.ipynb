{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction as in Apache Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import S\n",
    "\n",
    "sc = SparkContext(appName=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\twget --quiet https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -O $HOME/conda.sh',\n",
       " '\\tchmod 777 $HOME/conda.sh',\n",
       " '\\t/bin/bash /$HOME/conda.sh -b -p $HOME/conda',\n",
       " '\\tchmod -R 777 $HOME/conda',\n",
       " \"\\techo -e '\\\\nexport PATH=$HOME/conda/bin:$PATH' >> $HOME/.bashrc && source $HOME/.bashrc\",\n",
       " '\\tconda install -y pyarrow pyspark',\n",
       " '',\n",
       " \"#sudo echo -e '\\\\nexport PYSPARK_PYTHON=/home/hadoop/conda/bin/python' >> /etc/spark/conf/spark-env.sh\",\n",
       " '#sudo sed -i \"s/## region=us-east-1/region=us-east-2/\" /etc/hue/conf.empty/hue.ini',\n",
       " '',\n",
       " '',\n",
       " 'sudo pyspark --packages org.apache.spark:spark-avro:2.4.0',\n",
       " '',\n",
       " 'This failed to launch !!!!',\n",
       " '',\n",
       " '',\n",
       " '* How to know the large_temp_2.csv FOLDER size?',\n",
       " '    * **Ans**: hadoop fs -du -s -h /user/livy/large_temp_2.csv',\n",
       " '\\t\\t* 34.2 M  /user/livy/large_temp_2.csv',\n",
       " '',\n",
       " '* can 2 jobs, one for append and one for count work in parallel ? ',\n",
       " '* what is the read and count for csv file?',\n",
       " '* what is the read and count for parquete file?',\n",
       " '* can panda hold the csv or parquet file ?',\n",
       " '    * if yes, can it operate on a \"query\" with the same speed as that of spark dataframe?',\n",
       " '        * obviously not, otherwise spark should have closed its shutter !!! but let us see the difference and hail spark',\n",
       " '    * if no, can we slice the dataset using spark df and may be panda simply does the plot( then why not matplot lib?)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter == Query in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b53b3e56d197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtextFile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"abc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "textFile[textFile.value.contains(\"abc\")].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note that the above case has created RDD and dataframe because the file has been read using spark context and not SQL context\n",
    "\n",
    "Let us create sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "spark = SQLContext(sc).sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.read.text(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='\\twget --quiet https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -O $HOME/conda.sh')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[text.value.contains(\"get\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating new column in spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|wordCount|\n",
      "+---------+\n",
      "|        6|\n",
      "|        4|\n",
      "|        6|\n",
      "|        5|\n",
      "|       10|\n",
      "|        6|\n",
      "|        1|\n",
      "|        7|\n",
      "|        6|\n",
      "|        1|\n",
      "|        1|\n",
      "|        4|\n",
      "|        1|\n",
      "|        5|\n",
      "|        1|\n",
      "|        1|\n",
      "|        8|\n",
      "|        9|\n",
      "|        5|\n",
      "|        1|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text1 = text.select(size(split(text.value,\"\\s+\")).name(\"wordCount\"))\n",
    "text1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|max(wordCount)|\n",
      "+--------------+\n",
      "|            25|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.select(size(split(text.value,\"\\s+\")).name(\"wordCount\")).agg(max(col(\"wordCount\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulating MapReduce action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word=['', 'wget', '--quiet', 'https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh', '-O', '$HOME/conda.sh']),\n",
       " Row(word=['', 'chmod', '777', '$HOME/conda.sh']),\n",
       " Row(word=['', '/bin/bash', '/$HOME/conda.sh', '-b', '-p', '$HOME/conda']),\n",
       " Row(word=['', 'chmod', '-R', '777', '$HOME/conda']),\n",
       " Row(word=['', 'echo', '-e', \"'\\\\nexport\", \"PATH=$HOME/conda/bin:$PATH'\", '>>', '$HOME/.bashrc', '&&', 'source', '$HOME/.bashrc']),\n",
       " Row(word=['', 'conda', 'install', '-y', 'pyarrow', 'pyspark']),\n",
       " Row(word=['']),\n",
       " Row(word=['#sudo', 'echo', '-e', \"'\\\\nexport\", \"PYSPARK_PYTHON=/home/hadoop/conda/bin/python'\", '>>', '/etc/spark/conf/spark-env.sh']),\n",
       " Row(word=['#sudo', 'sed', '-i', '\"s/##', 'region=us-east-1/region=us-east-2/\"', '/etc/hue/conf.empty/hue.ini']),\n",
       " Row(word=['']),\n",
       " Row(word=['']),\n",
       " Row(word=['sudo', 'pyspark', '--packages', 'org.apache.spark:spark-avro:2.4.0']),\n",
       " Row(word=['']),\n",
       " Row(word=['This', 'failed', 'to', 'launch', '!!!!']),\n",
       " Row(word=['']),\n",
       " Row(word=['']),\n",
       " Row(word=['*', 'How', 'to', 'know', 'the', 'large_temp_2.csv', 'FOLDER', 'size?']),\n",
       " Row(word=['', '*', '**Ans**:', 'hadoop', 'fs', '-du', '-s', '-h', '/user/livy/large_temp_2.csv']),\n",
       " Row(word=['', '*', '34.2', 'M', '/user/livy/large_temp_2.csv']),\n",
       " Row(word=['']),\n",
       " Row(word=['*', 'can', '2', 'jobs,', 'one', 'for', 'append', 'and', 'one', 'for', 'count', 'work', 'in', 'parallel', '?', '']),\n",
       " Row(word=['*', 'what', 'is', 'the', 'read', 'and', 'count', 'for', 'csv', 'file?']),\n",
       " Row(word=['*', 'what', 'is', 'the', 'read', 'and', 'count', 'for', 'parquete', 'file?']),\n",
       " Row(word=['*', 'can', 'panda', 'hold', 'the', 'csv', 'or', 'parquet', 'file', '?']),\n",
       " Row(word=['', '*', 'if', 'yes,', 'can', 'it', 'operate', 'on', 'a', '\"query\"', 'with', 'the', 'same', 'speed', 'as', 'that', 'of', 'spark', 'dataframe?']),\n",
       " Row(word=['', '*', 'obviously', 'not,', 'otherwise', 'spark', 'should', 'have', 'closed', 'its', 'shutter', '!!!', 'but', 'let', 'us', 'see', 'the', 'difference', 'and', 'hail', 'spark']),\n",
       " Row(word=['', '*', 'if', 'no,', 'can', 'we', 'slice', 'the', 'dataset', 'using', 'spark', 'df', 'and', 'may', 'be', 'panda', 'simply', 'does', 'the', 'plot(', 'then', 'why', 'not', 'matplot', 'lib?)'])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.select(split(text.value,\"\\s+\").name(\"word\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|[, *, if, yes,, c...|    1|\n",
      "|[#sudo, sed, -i, ...|    1|\n",
      "|[, *, obviously, ...|    1|\n",
      "|[#sudo, echo, -e,...|    1|\n",
      "|[, *, **Ans**:, h...|    1|\n",
      "|[*, what, is, the...|    1|\n",
      "|[, /bin/bash, /$H...|    1|\n",
      "|[, echo, -e, '\\ne...|    1|\n",
      "|[sudo, pyspark, -...|    1|\n",
      "|[*, How, to, know...|    1|\n",
      "|[, wget, --quiet,...|    1|\n",
      "|[, chmod, 777, $H...|    1|\n",
      "|[This, failed, to...|    1|\n",
      "|[, chmod, -R, 777...|    1|\n",
      "|[*, what, is, the...|    1|\n",
      "|[, *, if, no,, ca...|    1|\n",
      "|[*, can, panda, h...|    1|\n",
      "|[*, can, 2, jobs,...|    1|\n",
      "|[, *, 34.2, M, /u...|    1|\n",
      "|                  []|    7|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " text.select(split(text.value,\"\\s+\").name(\"word\")).groupby(\"word\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explode, GroupBy and OrderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = text.select(explode(split(text.value,\"\\S +\")).name(\"word\")).groupby(\"word\").count().orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  fo|    4|\n",
      "|                  ca|    4|\n",
      "|                   o|    3|\n",
      "|                    |    3|\n",
      "|                spar|    3|\n",
      "|                coun|    3|\n",
      "|               \tchmo|    2|\n",
      "|                 wha|    2|\n",
      "|            '\\nexpor|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|                  no|    2|\n",
      "|                pand|    2|\n",
      "|                  cs|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|               \tchmo|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|            '\\nexpor|    2|\n",
      "|                  cs|    2|\n",
      "|                  on|    2|\n",
      "|                  no|    2|\n",
      "|                 wha|    2|\n",
      "|                  77|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  fo|    4|\n",
      "|                  ca|    4|\n",
      "|                spar|    3|\n",
      "|                   o|    3|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|               \tchmo|    2|\n",
      "|                 wha|    2|\n",
      "|                  no|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|                pand|    2|\n",
      "|                  cs|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+-----+\n",
      "|          word|count|\n",
      "+--------------+-----+\n",
      "|              |   17|\n",
      "|             -|   10|\n",
      "|            th|    8|\n",
      "|             i|    6|\n",
      "|            an|    5|\n",
      "|            ca|    4|\n",
      "|            fo|    4|\n",
      "|          coun|    3|\n",
      "|              |    3|\n",
      "|          spar|    3|\n",
      "|             o|    3|\n",
      "|            on|    2|\n",
      "|            cs|    2|\n",
      "|          pand|    2|\n",
      "|         \tchmo|    2|\n",
      "|$HOME/conda.sh|    2|\n",
      "|           wha|    2|\n",
      "|      '\\nexpor|    2|\n",
      "|            77|    2|\n",
      "|            no|    2|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+-----+\n",
      "|          word|count|\n",
      "+--------------+-----+\n",
      "|              |   17|\n",
      "|             -|   10|\n",
      "|            th|    8|\n",
      "|             i|    6|\n",
      "|            an|    5|\n",
      "|            ca|    4|\n",
      "|            fo|    4|\n",
      "|          spar|    3|\n",
      "|          coun|    3|\n",
      "|              |    3|\n",
      "|             o|    3|\n",
      "|            on|    2|\n",
      "|            cs|    2|\n",
      "|           wha|    2|\n",
      "|         \tchmo|    2|\n",
      "|$HOME/conda.sh|    2|\n",
      "|          pand|    2|\n",
      "|            77|    2|\n",
      "|            no|    2|\n",
      "|      '\\nexpor|    2|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                spar|    3|\n",
      "|                   o|    3|\n",
      "|                coun|    3|\n",
      "|                  on|    2|\n",
      "|                  77|    2|\n",
      "|                pand|    2|\n",
      "|                  cs|    2|\n",
      "|               \tchmo|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|            '\\nexpor|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  no|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                spar|    3|\n",
      "|                   o|    3|\n",
      "|                  on|    2|\n",
      "|                  cs|    2|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|            '\\nexpor|    2|\n",
      "|                  77|    2|\n",
      "|                  no|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                   o|    3|\n",
      "|                coun|    3|\n",
      "|                    |    3|\n",
      "|                spar|    3|\n",
      "|/user/livy/large_...|    2|\n",
      "|                 wha|    2|\n",
      "|                pand|    2|\n",
      "|               \tchmo|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|            '\\nexpor|    2|\n",
      "|                  on|    2|\n",
      "|                  no|    2|\n",
      "|                  cs|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "1.44 s ± 157 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  fo|    4|\n",
      "|                  ca|    4|\n",
      "|                   o|    3|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                spar|    3|\n",
      "|               \tchmo|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  no|    2|\n",
      "|                  cs|    2|\n",
      "|                 wha|    2|\n",
      "|                pand|    2|\n",
      "|                  77|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  on|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importance of Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|                  cs|    2|\n",
      "|                  77|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  no|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|                  cs|    2|\n",
      "|                  77|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  no|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|                  cs|    2|\n",
      "|                  77|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  no|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|                  cs|    2|\n",
      "|                  77|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  no|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|                  cs|    2|\n",
      "|                  77|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  no|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|                  cs|    2|\n",
      "|                  77|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  no|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|                  cs|    2|\n",
      "|                  77|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  no|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |   17|\n",
      "|                   -|   10|\n",
      "|                  th|    8|\n",
      "|                   i|    6|\n",
      "|                  an|    5|\n",
      "|                  ca|    4|\n",
      "|                  fo|    4|\n",
      "|                    |    3|\n",
      "|                coun|    3|\n",
      "|                   o|    3|\n",
      "|                spar|    3|\n",
      "|                 wha|    2|\n",
      "|               \tchmo|    2|\n",
      "|                  cs|    2|\n",
      "|                  77|    2|\n",
      "|/user/livy/large_...|    2|\n",
      "|                  on|    2|\n",
      "|      $HOME/conda.sh|    2|\n",
      "|                  no|    2|\n",
      "|            '\\nexpor|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "184 ms ± 15.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One more way of creating spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark2 = SparkSession.builder.appName(\"temp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|\twget --quiet htt...|\n",
      "|\tchmod 777 $HOME/...|\n",
      "|\t/bin/bash /$HOME...|\n",
      "|\tchmod -R 777 $HO...|\n",
      "|\techo -e '\\nexpor...|\n",
      "|\tconda install -y...|\n",
      "|                    |\n",
      "|#sudo echo -e '\\n...|\n",
      "|#sudo sed -i \"s/#...|\n",
      "|                    |\n",
      "|                    |\n",
      "|sudo pyspark --pa...|\n",
      "|                    |\n",
      "|This failed to la...|\n",
      "|                    |\n",
      "|                    |\n",
      "|* How to know the...|\n",
      "|    * **Ans**: ha...|\n",
      "|\t\t* 34.2 M  /user...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark2.read.text(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Note on Cluster and spark program execution on them\n",
    "* one single python spark program has 2 logical components.\n",
    "    * Driver tasks\n",
    "        * To assgin a task and collect the task operation output.\n",
    "        * It cordianates with resource managers like yarn and mesos for resource availing. This is transperant to the programmer as he does only read, write and dataframe related operations. \n",
    "    * Worker tasks\n",
    "        * indicates its presence to resource allocator and hence gets nominated for a task\n",
    "        * executes the instructions as per driver program\n",
    "        * note that here operation is limited to the data available in its scope and hence execution is faster and also parallel processing is feasible.\n",
    "        \n",
    "        \n",
    "* Monitoring can be done through: http://<driver-node>:4040\n",
    "* There are application like livy which also provides interactive access to jobs and thier features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Job Scheduling\n",
    "* unlike, in any other scheduling context, here we are talking about the resource scheduling\n",
    "\n",
    "\n",
    "* Static Partitioning:\n",
    "    * number of resources to a job can be limited through spark submit arguments\n",
    "        * --num-executors : number of executors for a job\n",
    "        * --executor-memory\n",
    "        * --executor-cores : Note cores are logical units of computation in CPU\n",
    "\n",
    "\n",
    "\n",
    "* Dynamic Resource Allocation:\n",
    "    * Jobs can return the resource after current usage and request again for later use.\n",
    "    * set spark.dynamicAllocation.enabled to true\n",
    "    * set spark.shuffle.service.enabled to true\n",
    "        * enabling resource executor to be removed but not the shuffle files. So intermediate results are retained so that when executor requires it, can be fetched. \n",
    "        * Shuffle service which always run and can collect all the shuffle files info across application can avoid a scenario of new executor trying to access the old executor shuffule file in progress writing.\n",
    "            * Here Old executor can submit a shuffle right to the shuffle services and terminate gracefully.\n",
    "            * Shuffle service will handle the situation of new executor requesting the old shuffle file content.\n",
    "\n",
    "    * spark.dynamicAllocation.schedulerBacklogTimeout is used to trigger the request.\n",
    "        * if not availed per first request, it will request again for every spark.dynamicAllocation.sustainedSchedulerBacklogTimeout seconds. \n",
    "        * executor count is increased exponentially for every subsequent request\n",
    "            * this is because job was waiting for it for so long and has to catch up for the waiting period\n",
    "            * it also acts as a buffer for future use (as the intial request did not get fulfill).\n",
    "            \n",
    "    * executors are removed after spark.dynamicAllocation.executorIdleTimeout seconds.\n",
    "            \n",
    "        \n",
    "        \n",
    "* Scheduling Within an Application:\n",
    "    * Fair scheduling: all jobs within application like save, collect etc are by default executed FIFO manner but Fair scheduling can allocate resources to smaller jobs even wen long jobs are being execute.\n",
    "        * conf.set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "        * Pools can also have \n",
    "            * weights to decide which has to be given higher preference\n",
    "            * minShare to decide which is the minimum share despite being lower weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
