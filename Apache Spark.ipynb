{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction as in Apache Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\twget --quiet https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -O $HOME/conda.sh',\n",
       " '\\tchmod 777 $HOME/conda.sh',\n",
       " '\\t/bin/bash /$HOME/conda.sh -b -p $HOME/conda',\n",
       " '\\tchmod -R 777 $HOME/conda',\n",
       " \"\\techo -e '\\\\nexport PATH=$HOME/conda/bin:$PATH' >> $HOME/.bashrc && source $HOME/.bashrc\",\n",
       " '\\tconda install -y pyarrow pyspark',\n",
       " '',\n",
       " \"#sudo echo -e '\\\\nexport PYSPARK_PYTHON=/home/hadoop/conda/bin/python' >> /etc/spark/conf/spark-env.sh\",\n",
       " '#sudo sed -i \"s/## region=us-east-1/region=us-east-2/\" /etc/hue/conf.empty/hue.ini',\n",
       " '',\n",
       " '',\n",
       " 'sudo pyspark --packages org.apache.spark:spark-avro:2.4.0',\n",
       " '',\n",
       " 'This failed to launch !!!!',\n",
       " '',\n",
       " '',\n",
       " '* How to know the large_temp_2.csv FOLDER size?',\n",
       " '    * **Ans**: hadoop fs -du -s -h /user/livy/large_temp_2.csv',\n",
       " '\\t\\t* 34.2 M  /user/livy/large_temp_2.csv',\n",
       " '',\n",
       " '* can 2 jobs, one for append and one for count work in parallel ? ',\n",
       " '* what is the read and count for csv file?',\n",
       " '* what is the read and count for parquete file?',\n",
       " '* can panda hold the csv or parquet file ?',\n",
       " '    * if yes, can it operate on a \"query\" with the same speed as that of spark dataframe?',\n",
       " '        * obviously not, otherwise spark should have closed its shutter !!! but let us see the difference and hail spark',\n",
       " '    * if no, can we slice the dataset using spark df and may be panda simply does the plot( then why not matplot lib?)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
