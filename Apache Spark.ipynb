{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction as in Apache Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import S\n",
    "\n",
    "sc = SparkContext(appName=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\twget --quiet https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -O $HOME/conda.sh',\n",
       " '\\tchmod 777 $HOME/conda.sh',\n",
       " '\\t/bin/bash /$HOME/conda.sh -b -p $HOME/conda',\n",
       " '\\tchmod -R 777 $HOME/conda',\n",
       " \"\\techo -e '\\\\nexport PATH=$HOME/conda/bin:$PATH' >> $HOME/.bashrc && source $HOME/.bashrc\",\n",
       " '\\tconda install -y pyarrow pyspark',\n",
       " '',\n",
       " \"#sudo echo -e '\\\\nexport PYSPARK_PYTHON=/home/hadoop/conda/bin/python' >> /etc/spark/conf/spark-env.sh\",\n",
       " '#sudo sed -i \"s/## region=us-east-1/region=us-east-2/\" /etc/hue/conf.empty/hue.ini',\n",
       " '',\n",
       " '',\n",
       " 'sudo pyspark --packages org.apache.spark:spark-avro:2.4.0',\n",
       " '',\n",
       " 'This failed to launch !!!!',\n",
       " '',\n",
       " '',\n",
       " '* How to know the large_temp_2.csv FOLDER size?',\n",
       " '    * **Ans**: hadoop fs -du -s -h /user/livy/large_temp_2.csv',\n",
       " '\\t\\t* 34.2 M  /user/livy/large_temp_2.csv',\n",
       " '',\n",
       " '* can 2 jobs, one for append and one for count work in parallel ? ',\n",
       " '* what is the read and count for csv file?',\n",
       " '* what is the read and count for parquete file?',\n",
       " '* can panda hold the csv or parquet file ?',\n",
       " '    * if yes, can it operate on a \"query\" with the same speed as that of spark dataframe?',\n",
       " '        * obviously not, otherwise spark should have closed its shutter !!! but let us see the difference and hail spark',\n",
       " '    * if no, can we slice the dataset using spark df and may be panda simply does the plot( then why not matplot lib?)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter == Query in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b53b3e56d197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtextFile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"abc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "textFile[textFile.value.contains(\"abc\")].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note that the above case has created RDD and dataframe because the file has been read using spark context and not SQL context\n",
    "\n",
    "Let us create sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "spark = SQLContext(sc).sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.read.text(r\"file:///C:\\Users\\Dell\\Desktop\\Hadoop\\Spark\\bootStrap.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='\\twget --quiet https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -O $HOME/conda.sh')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[text.value.contains(\"get\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating new column in spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|wordCount|\n",
      "+---------+\n",
      "|        6|\n",
      "|        4|\n",
      "|        6|\n",
      "|        5|\n",
      "|       10|\n",
      "|        6|\n",
      "|        1|\n",
      "|        7|\n",
      "|        6|\n",
      "|        1|\n",
      "|        1|\n",
      "|        4|\n",
      "|        1|\n",
      "|        5|\n",
      "|        1|\n",
      "|        1|\n",
      "|        8|\n",
      "|        9|\n",
      "|        5|\n",
      "|        1|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = text.select(size(split(text.value,\"\\s+\")).name(\"wordCount\"))\n",
    "text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|max(wordCount)|\n",
      "+--------------+\n",
      "|            25|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.agg(max(text.wordCount)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
