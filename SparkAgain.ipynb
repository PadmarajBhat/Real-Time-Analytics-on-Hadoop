{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkAgain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadmarajBhat/Real-Time-Analytics-on-Hadoop/blob/master/SparkAgain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FP7oEfb1g7j",
        "colab_type": "text"
      },
      "source": [
        "what is Big Data?\n",
        "-\tIt is a problem statement and the platform like hadoop provide an architecture to develop massively parallel and scalable application. Hadoop introduced HDFS which involves maintaininng the distributed file system. In the later version of distributed computing, spark, was introduced. It provides scalability, data paralellism and fault tolerance.\n",
        "\n",
        "\n",
        "What are the alternatives to Hadoop?\n",
        "Hydra, DataTorrent RTS, Google BigQuery and Mesos\n",
        "\n",
        "\n",
        "Spark fails if master node gets data (through take or collect statements) out of its size. Idea is to compile data at worker node and get only the summary of it in the master node.\n",
        "\n",
        "Spark does lazy evaluations : stores the mapping or transformations on data and uses it only when there is requirement for transformed data. This stratergy helps in fault tolerance too.\n",
        "\n",
        "It can run anywhere like yarn, mesos, stand alone or cluster, also on kubernetes.\n",
        "\n",
        "File Systems: parquet(has serialization and deserialization logic and hence slow writing), avro (supports shema evaluation), ORC(index data stores the offset to row data and hence faster) ,csv, text, json etc\n",
        "\n",
        "Streaming: Process the data as and when it arrives. More suited for ETL or model predictions. It is micro batching in spark.\n",
        "* Output modes are complete, append(selection, filtering and baisc transformation) and update(with aggregation).\n",
        "* Window: set of data is determined by 2 ends of time intervals. If these time interval is increased then there is a probability of increase in set of data and if reduced gap between 2 intervals might  reduce size of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0jm1mIoXZto",
        "colab_type": "text"
      },
      "source": [
        "Pandas UDF (user defined functions): uses apache arrow for smoother transformation to pandas and thus avail pandas functionalities on spark dataframe. \n",
        "\n",
        "Note that ther is UDFs too which can work on spark dataframe which can be applied on the row data in spark dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJcwvRnPg64F",
        "colab_type": "text"
      },
      "source": [
        "Machine learning: There are enough list of regression and classification and clustering methods under Spark MLlib which works on data frames. However, for deep learnin tensorflowonSpark or keras can be used for distributed training and modelling. \n",
        "\n",
        "\n",
        "\n",
        "TensorflowOnSpark: https://yahoohadoop.tumblr.com/post/157196317141/open-sourcing-tensorflowonspark-distributed-deep\n",
        "  *  the worker nodes uses either gRPC (https://grpc.io/docs/guides/) or RDMA(Remote direct memory) to  communicate the parameter update between the worker node.\n",
        "  * Data into the tensors can be through \n",
        "    * TensorFlow Queuerunners where tensorflow directly access the HDFS files\n",
        "    * or throug Spark Feed where spark rdd is fed to worker and worker passes the data into tensor through feed_dict.\n",
        "    \n",
        "    * API:\n",
        "      * TFCluster : TFCluster.run indicates the core/main function which does tensorflow activities and also can option send the spark submit command line arguments\n",
        "      * TFNode: TFNode.start_cluster_server is called at the beginning of the core/main function which would take ctx input from TFCluster and kick start the tf executions.\n",
        "      \n",
        "      * examples indicate that there can be no parameter server. Raised a question on the same. https://stackoverflow.com/questions/56469814/can-tensorflowonspark-run-without-parameter-server\n",
        "\n",
        "elephas: Keras on Spark\n",
        "\n",
        "Both supports asynchronous and synchronous training.\n",
        " * As explained in the link: https://stackoverflow.com/a/34361377/8693106\n",
        "    * Synchronous learning update the average gradients to the weights from each worker node.\n",
        "      * In my opinion, in all cases where the worker nodes do not have equibalanced batch of dataset, synchronous learning has to be adopted.\n",
        "    * Asynchronous learning lets worker override the parameter update from one another.\n",
        "      * Again in my opinion, cases where all workers or of same config and has balanced data set can work in this mode of learning because all worker nodes would have the more or less same parameter update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mC855y1gEfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}