{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkAgain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadmarajBhat/Real-Time-Analytics-on-Hadoop/blob/master/SparkAgain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FP7oEfb1g7j",
        "colab_type": "text"
      },
      "source": [
        "what is Big Data?\n",
        "-\tIt is a problem statement and the platform like hadoop provide an architecture to develop massively parallel and scalable application. Hadoop introduced HDFS which involves maintaininng the distributed file system. In the later version of distributed computing, spark, was introduced. It provides scalability, data paralellism and fault tolerance.\n",
        "\n",
        "\n",
        "What are the alternatives to Hadoop?\n",
        "Hydra, DataTorrent RTS, Google BigQuery and Mesos\n",
        "\n",
        "\n",
        "Spark fails if master node gets data (through take or collect statements) out of its size. Idea is to compile data at worker node and get only the summary of it in the master node.\n",
        "\n",
        "Spark does lazy evaluations : stores the mapping or transformations on data and uses it only when there is requirement for transformed data. This stratergy helps in fault tolerance too.\n",
        "\n",
        "It can run anywhere like yarn, mesos, stand alone or cluster, also on kubernetes.\n",
        "\n",
        "File Systems: parquet(has serialization and deserialization logic and hence slow writing), avro (supports shema evaluation), ORC(index data stores the offset to row data and hence faster) ,csv, text, json etc\n",
        "\n",
        "Streaming: Process the data as and when it arrives. More suited for ETL or model predictions. It is micro batching in spark.\n",
        "* Output modes are complete, append(selection, filtering and baisc transformation) and update(with aggregation).\n",
        "* Window: set of data is determined by 2 ends of time intervals. If these time interval is increased then there is a probability of increase in set of data and if reduced gap between 2 intervals might  reduce size of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m8eVdbw1clt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}